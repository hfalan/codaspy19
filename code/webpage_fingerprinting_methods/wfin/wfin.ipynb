{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wfin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wfin.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "sys.path.insert(0, os.path.join(cur_dir,\"utils\"))\n",
    "import dataset_new as dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import Config\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import tld\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def get_tld(hostname):\n",
    "    top_level_hostname = tld.get_tld(hostname, fix_protocol=True, fail_silently=True)\n",
    "    return top_level_hostname if top_level_hostname else \"unknown\"\n",
    "\n",
    "class Wfin():\n",
    "    @classmethod\n",
    "    def get_name(cls):\n",
    "        return \"Wfin\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.server_addresses = set()\n",
    "        self.host_names = set()\n",
    "    \n",
    "    def get_features(self, visit_file):\n",
    "        Config.hostname = dict()\n",
    "        features = dataset.get_features(visit_file, SERVER_ADDRESS=self.server_addresses, HOST_NAMES=self.host_names)        \n",
    "        return features\n",
    "    \n",
    "    def get_features_all(self, visit_files, n_cpu):\n",
    "        with Pool(n_cpu) as pool:\n",
    "            return list(pool.map(self.get_features, visit_files, chunksize=1))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_host_names_and_server_addresses(self, visit_file):\n",
    "        with open(visit_file) as f:\n",
    "            visit = json.load(f)\n",
    "            \n",
    "        server_addresses = set(visit['ip_to_name'].keys())\n",
    "        host_names = set(visit['ip_to_name'].values())\n",
    "        return server_addresses, host_names\n",
    "            \n",
    "    def determine_host_names_and_server_address(self, visit_files, n_cpu):\n",
    "        with Pool(n_cpu) as pool:\n",
    "            for server_addresses, host_names in list(pool.map(self.get_host_names_and_server_addresses, visit_files)):\n",
    "                self.server_addresses |= server_addresses\n",
    "                self.host_names |= set(map(get_tld, host_names))\n",
    "            \n",
    "    def classify(self, train_visit_files, test_visit_files, visit_file_label, output_dir, n_cpu):\n",
    "        print(\"determine host names2\")\n",
    "        print(len(train_visit_files), len(test_visit_files))\n",
    "        self.determine_host_names_and_server_address(train_visit_files, n_cpu)\n",
    "#         print(len(self.server_addresses), list(self.server_addresses)[:5])\n",
    "#         print(len(self.host_names), list(self.host_names)[:5])\n",
    "\n",
    "\n",
    "        print(\"extract training features\")\n",
    "        train_x = self.get_features_all(train_visit_files, n_cpu)\n",
    "        train_y = list([visit_file_label[x] for x in train_visit_files])\n",
    "        \n",
    "        print(\"extract test features\")\n",
    "        test_x = self.get_features_all(test_visit_files, n_cpu)\n",
    "        test_y = list([visit_file_label[x] for x in test_visit_files])\n",
    "        \n",
    "        print(\"fitting 600\")\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorize', DictVectorizer()),\n",
    "#             (\"classify\", ExtraTreesClassifier(n_jobs=n_cpu, n_estimators=2000, min_samples_leaf=4))\n",
    "            (\"classify\", ExtraTreesClassifier(n_jobs=n_cpu, n_estimators=1000))\n",
    "        ])\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        clf = pipeline.named_steps['classify']\n",
    "        importances = clf.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "                     axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        feature_names = pipeline.named_steps['vectorize'].feature_names_\n",
    "        with open(os.path.join(output_dir, \"feature_names.pickle\"),\"wb\") as f:\n",
    "            pickle.dump(list(feature_names), f)\n",
    "            \n",
    "        print(\"num features:\", len(feature_names))\n",
    "        \n",
    "        f_importance = defaultdict(float)\n",
    "        for i in indices:\n",
    "            f_importance[feature_names[i].split(\"*\")[0]]+=importances[i]\n",
    "            \n",
    "        print(\"dump feature importance\")\n",
    "        print(output_dir)\n",
    "        with open(os.path.join(output_dir, \"feature_importance.pickle\"),\"wb\") as f:\n",
    "            pickle.dump(f_importance, f)\n",
    "            \n",
    "        for k, v in sorted(f_importance.items(), key=lambda x: x[1],reverse=True)[:5]:\n",
    "            print(k, v)\n",
    "        \n",
    "        \n",
    "        score = pipeline.score(test_x, test_y)\n",
    "        \n",
    "        try:\n",
    "            probabilities = pipeline.predict_proba(test_x)\n",
    "            probabilities_file = os.path.join(output_dir,\"probabilities.txt\")\n",
    "            with open(probabilities_file,\"w\") as f:\n",
    "                f.write(\"labels {}\\n\".format(\" \".join(map(lambda x: str(int(x)), pipeline.named_steps['classify'].classes_))))\n",
    "                for i, label in enumerate(test_y):\n",
    "                    f.write(\"{} {}\\n\".format(int(label), \" \".join(map(lambda x: str(x), probabilities[i]))))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "                   \n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
