{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cumul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cumul.py\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import numpy\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Cumul():\n",
    "    @classmethod\n",
    "    def get_name(cls):\n",
    "        return \"CUMUL\"\n",
    "    \n",
    "    def get_cumul_features(self, packet_sizes):\n",
    "        #authors' implementation\n",
    "        features = []\n",
    "        \n",
    "        total = []\n",
    "        cum = []\n",
    "        pos = []\n",
    "        neg = []\n",
    "        inSize = 0\n",
    "        outSize = 0\n",
    "        inCount = 0\n",
    "        outCount = 0\n",
    "        for packetsize in packet_sizes:\n",
    "            if packetsize > 0:\n",
    "                inSize += packetsize\n",
    "                inCount += 1\n",
    "                # cumulated packetsizes\n",
    "                if len(cum) == 0:\n",
    "                    cum.append(packetsize)\n",
    "                    total.append(packetsize)\n",
    "                    pos.append(packetsize)\n",
    "                    neg.append(0)\n",
    "                else:\n",
    "                    cum.append(cum[-1] + packetsize)\n",
    "                    total.append(total[-1] + abs(packetsize))\n",
    "                    pos.append(pos[-1] + packetsize)\n",
    "                    neg.append(neg[-1] + 0)\n",
    "\n",
    "            # outgoing packets\n",
    "            if packetsize < 0:\n",
    "                outSize += abs(packetsize)\n",
    "                outCount += 1\n",
    "                if len(cum) == 0:\n",
    "                    cum.append(packetsize)\n",
    "                    total.append(abs(packetsize))\n",
    "                    pos.append(0)\n",
    "                    neg.append(abs(packetsize))\n",
    "                else:\n",
    "                    cum.append(cum[-1] + packetsize)\n",
    "                    total.append(total[-1] + abs(packetsize))\n",
    "                    pos.append(pos[-1] + 0)\n",
    "                    neg.append(neg[-1] + abs(packetsize))\n",
    "\n",
    "        # add feature\n",
    "#         features.append(classLabel)\n",
    "        features.append(inCount)\n",
    "        features.append(outCount)\n",
    "        features.append(outSize)\n",
    "        features.append(inSize)\n",
    "        \n",
    "        featureCount = 100\n",
    "        cumFeatures = numpy.interp(numpy.linspace(total[0], total[-1], featureCount+1), total, cum)\n",
    "        for el in itertools.islice(cumFeatures, 1, None):\n",
    "            features.append(el)\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def get_x(self, visit_file):\n",
    "        with open(visit_file) as f:\n",
    "            visit = json.load(f)\n",
    "            \n",
    "        packets = []\n",
    "        for connection in visit['tcp_connections']:\n",
    "            for packet in connection['packets']:\n",
    "                packets.append(packet)\n",
    "\n",
    "        packets.sort(key=lambda packet: packet[0])\n",
    "        packet_sizes = []\n",
    "\n",
    "        for packet in packets:\n",
    "            if not packet[1]:\n",
    "                continue\n",
    "            packet_sizes.append(packet[1])\n",
    "            \n",
    "        features = self.get_cumul_features(packet_sizes)\n",
    "        return features\n",
    "    \n",
    "    def get_x_all(self, visit_files, n_cpu):\n",
    "        with Pool(n_cpu) as pool:\n",
    "            return list(pool.map(self.get_x, visit_files))\n",
    "        \n",
    "    def tune_parameters_and_get_score(self, train_x, train_y, test_x, test_y, n_cpu, output_dir):\n",
    "        pipeline = Pipeline([\n",
    "            ('scale', StandardScaler()),\n",
    "            ('classify', SVC(probability=True))\n",
    "        ])\n",
    "        \n",
    "        param_grid = [\n",
    "            {\n",
    "                'classify__C': [2**i for i in range(11,17)],\n",
    "                'classify__gamma': [2**i for i in range(-3,3)]\n",
    "            }\n",
    "        ]\n",
    "                        \n",
    "        cv = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=1)\n",
    "        \n",
    "        #error_score=0 : Value to assign to the score if an error occurs in estimator fitting\n",
    "        grid = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=n_cpu, refit=True, error_score=0,verbose=0)\n",
    "        grid.fit(train_x, train_y)\n",
    "        print(\"The best parameters are %s with a score of %0.2f\"\n",
    "              % (grid.best_params_, grid.best_score_))  \n",
    "        \n",
    "        probabilities = grid.predict_proba(test_x)\n",
    "        probabilities_file = os.path.join(output_dir,\"probabilities.txt\")\n",
    "        with open(probabilities_file,\"w\") as f:\n",
    "            f.write(\"labels {}\\n\".format(\" \".join(map(lambda x: str(int(x)), sorted(set(train_y))))))\n",
    "            for i, label in enumerate(test_y):\n",
    "                f.write(\"{} {}\\n\".format(int(label), \" \".join(map(lambda x: str(x), probabilities[i]))))\n",
    "                \n",
    "        return grid.score(test_x, test_y)\n",
    "    \n",
    "    def get_score(self, train_x, train_y, test_x, test_y, n_cpu, output_dir):\n",
    "        pipeline = Pipeline([\n",
    "            ('scale', StandardScaler()),\n",
    "            ('classify', SVC(C=2**22, gamma=2**4))\n",
    "        ])\n",
    "        print(\"get_score\")\n",
    "        pipeline.fit(train_x, train_y)      \n",
    "        return pipeline.score(test_x, test_y)\n",
    "           \n",
    "    def classify(self, train_visit_files, test_visit_files, visit_file_label, output_dir, n_cpu):\n",
    "        train_x = self.get_x_all(train_visit_files, n_cpu)\n",
    "        train_y = list([visit_file_label[x] for x in train_visit_files])\n",
    "        \n",
    "        test_x = self.get_x_all(test_visit_files, n_cpu)\n",
    "        test_y = list([visit_file_label[x] for x in test_visit_files])\n",
    "        \n",
    "        score = self.tune_parameters_and_get_score(train_x, train_y, test_x, test_y, n_cpu, output_dir)\n",
    "#         score = self.get_score(train_x, train_y, test_x, test_y, n_cpu, output_dir)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
