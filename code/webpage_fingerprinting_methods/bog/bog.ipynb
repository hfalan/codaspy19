{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bog.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import os, sys\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "sys.path.insert(0, os.path.join(cur_dir,\"utils\"))\n",
    "from itertools import repeat\n",
    "import attack\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import scipy.sparse\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "def get_server_ip(tcp_connection_id, host_ip):\n",
    "    server_ip = None\n",
    "    for socket in tcp_connection_id.split(\"-\"):\n",
    "        ip = socket.split(\":\")[0]\n",
    "        if ip != host_ip:\n",
    "            server_ip = ip\n",
    "            break\n",
    "    return server_ip\n",
    "\n",
    "def get_burst_pairs(packet_size_sequence):\n",
    "    direction = -1\n",
    "    burst_pairs = []\n",
    "    outgoing = 0\n",
    "    incoming = 0\n",
    "    for size in packet_size_sequence:\n",
    "        if size < 0 and direction == 1:\n",
    "            burst_pairs.append((outgoing, incoming))\n",
    "            outgoing = 0\n",
    "            incoming = 0\n",
    "            direction = -1\n",
    "        if size > 0 and direction == -1:\n",
    "            direction = 1\n",
    "        if size < 0:\n",
    "            outgoing += -1*size\n",
    "        else:\n",
    "            incoming += size\n",
    "    burst_pairs.append((outgoing, incoming))\n",
    "\n",
    "    return burst_pairs\n",
    "\n",
    "def extract_features(visit):\n",
    "    host_burst_pairs = defaultdict(list)\n",
    "    packet_size_counter = Counter()\n",
    "\n",
    "#     host_ip = raw_visit_data['host_ip']\n",
    "    for connection in visit['tcp_connections']:\n",
    "        server_ip = get_server_ip(connection['connection_id'], visit['visit_log']['host_ip'])\n",
    "        server_host_name = visit['ip_to_name'].get(server_ip)\n",
    "        packet_size_sequence = []\n",
    "        for packet in connection['packets']:\n",
    "            tcp_len = packet[1]\n",
    "            if not tcp_len: continue\n",
    "            packet_size_sequence.append(packet[1])\n",
    "            packet_size_counter[packet[1]] += 1\n",
    "            \n",
    "        burst_pairs = get_burst_pairs(packet_size_sequence)\n",
    "        host_burst_pairs[server_host_name] += burst_pairs  \n",
    "\n",
    "    visit_features = {\n",
    "                \"host_burst_pairs\" : host_burst_pairs,\n",
    "                \"packet_size_counter\": packet_size_counter\n",
    "            }\n",
    "    return visit_features\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def load_visit(visit_file):\n",
    "    visit = load_json(visit_file)\n",
    "    return extract_features(visit)\n",
    "    \n",
    "def load_visit_features(visit_files, n_cpu):\n",
    "    with Pool(n_cpu) as pool:\n",
    "        visit_data = dict(zip(visit_files, pool.map(load_visit, visit_files)))     \n",
    "    return visit_data\n",
    "\n",
    "class BoG():\n",
    "    @classmethod\n",
    "    def get_name(cls):\n",
    "        return \"BoG\"\n",
    "    \n",
    "    def get_x(self, visit_file):\n",
    "        with open(visit_file) as f:\n",
    "            visit = json.load(f)\n",
    "        packet_sizes = []\n",
    "        for connection in visit['tcp_connections']:\n",
    "            packet_sizes += [x[1] for x in connection['packets']]\n",
    "        return Counter(packet_sizes)\n",
    "    \n",
    "    def get_x_all(self, visit_files, n_cpu):\n",
    "        with Pool(n_cpu) as pool:\n",
    "            return list(pool.map(self.get_x, visit_files))\n",
    "        \n",
    "    def load_raw_data(self, training_visits, test_visits, visit_label, output_dir):\n",
    "        ret = self.get_train_test_hold_lists(training_visits, test_visits, visit_label)\n",
    "        train_list, test_list, hold_list = ret\n",
    "        \n",
    "        TRAINING_DIR = os.path.join(output_dir, 'training_points')\n",
    "        TESTING_DIR = os.path.join(output_dir, 'testing_points')\n",
    "        HOLDOUT_DIR = os.path.join(output_dir, 'holdout_points')\n",
    "        DOMAIN_DIR = os.path.join(output_dir, 'domain_models')\n",
    "        \n",
    "        self.extract_points(train_list, TRAINING_DIR)\n",
    "        self.extract_points(test_list, TESTING_DIR)\n",
    "        self.extract_points(hold_list, HOLDOUT_DIR)\n",
    "        \n",
    "        # remove extraneous domains\n",
    "        for d in set(os.listdir(TRAINING_DIR)).difference(set(os.listdir(TESTING_DIR))):\n",
    "            os.remove(os.path.join(TRAINING_DIR, d))\n",
    "        for d in set(os.listdir(TESTING_DIR)).difference(set(os.listdir(TRAINING_DIR))):\n",
    "            os.remove(os.path.join(TESTING_DIR, d))\n",
    "        tmp = set(os.listdir(TESTING_DIR)).union(set(os.listdir(TRAINING_DIR)))\n",
    "        for d in set(os.listdir(HOLDOUT_DIR)).difference(tmp):\n",
    "            os.remove(os.path.join(HOLDOUT_DIR, d))\n",
    "        \n",
    "        domains = []\n",
    "        for d in os.listdir(TRAINING_DIR):\n",
    "            domain = attack.Domain(DOMAIN_DIR, d, TRAINING_DIR, TESTING_DIR, HOLDOUT_DIR)\n",
    "            domain.load_training_data()\n",
    "            domains.append(domain) \n",
    "            \n",
    "\n",
    "        \n",
    "        return domains, train_list, test_list, hold_list\n",
    "            \n",
    "    def classify(self, train_visit_files, test_visit_files, visit_file_label, output_dir, n_cpu):\n",
    "        training_visit_data = load_visit_features(train_visit_files, n_cpu)\n",
    "        test_visit_data = load_visit_features(test_visit_files, n_cpu)\n",
    "        self.visit_features = {**training_visit_data, **test_visit_data}\n",
    "        labels = set(visit_file_label.values())\n",
    "        label_to_id = dict(zip(labels, range(len(labels))))\n",
    "        visit_label_id = {visit:label_to_id[label] for visit,label in visit_file_label.items()}\n",
    "        ret_val = self.load_raw_data(train_visit_files, test_visit_files, visit_label_id, output_dir)\n",
    "        domains, train_list, test_list, hold_list = ret_val\n",
    "        \n",
    "        packet_size_features = self.get_packet_size_features(train_list, test_list, hold_list)\n",
    "        \n",
    "        attack.main(domains, train_list, test_list, hold_list, packet_size_features, output_dir, n_cpu)\n",
    "        \n",
    "        with open(os.path.join(output_dir, \"lr_accuracy\")) as f:\n",
    "            return float(f.readlines()[0].split()[-1])\n",
    "#         train_x = self.get_x_all(train_visit_files, n_cpu)\n",
    "#         train_y = list([visit_file_label[x] for x in train_visit_files])\n",
    "        \n",
    "#         test_x = self.get_x_all(test_visit_files, n_cpu)\n",
    "#         test_y = list([visit_file_label[x] for x in test_visit_files])\n",
    "\n",
    "#         classifier = Pipeline([\n",
    "#             ('vectorize', DictVectorizer()),\n",
    "#             (\"classify\", RandomForestClassifier(n_jobs=n_cpu, n_estimators=600))\n",
    "#         ])\n",
    "#         classifier.fit(train_x, train_y)\n",
    "#         score = classifier.score(test_x, test_y)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_server_ip(cls, tcp_connection_id, host_ip):\n",
    "        server_ip = None\n",
    "        for socket in tcp_connection_id.split(\"-\"):\n",
    "            ip = socket.split(\":\")[0]\n",
    "            if ip != host_ip:\n",
    "                server_ip = ip\n",
    "                break\n",
    "        return server_ip\n",
    "    \n",
    "    @classmethod\n",
    "    def get_train_test_hold_lists(cls, training_visits, test_visits, visit_label):\n",
    "        #Miller et al. use train, test, holdout terminology\n",
    "        #be consistent, split training_visits and crate a test dataset that will be used for validation\n",
    "        \n",
    "        label_visits = defaultdict(list)\n",
    "        for visit_id in training_visits:\n",
    "            label = visit_label[visit_id]\n",
    "            label_visits[label].append((visit_id, label))\n",
    "    \n",
    "        #determine train and test \n",
    "        #split training_visits to half\n",
    "        train_list = []\n",
    "        test_list = []\n",
    "        hold_list = []\n",
    "        for label, visits in label_visits.items():\n",
    "            sorted_visits = sorted(visits)\n",
    "            split_index = len(visits)//2 #use half of the training samples for validation\n",
    "            train_list += sorted_visits[:split_index] #first half\n",
    "            test_list += sorted_visits[split_index:]\n",
    "            \n",
    "        for visit_id in test_visits: \n",
    "            label = visit_label[visit_id]\n",
    "            hold_list.append((visit_id, label))\n",
    "        return train_list, test_list, hold_list\n",
    "    \n",
    "    def extract_points(self, sample_list, out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        data_points = self.load_all_sample_points(sample_list)\n",
    "        self.write_point_files(data_points, out_dir)\n",
    "        \n",
    "    def load_all_sample_points(self, sample_list):\n",
    "        \n",
    "#         print(self.visit_data[list(self.visit_data)[0]])\n",
    "        #pets2014_miller_code/code/bog/extract_points.py \n",
    "        #load_all_sample_points(sample_list, feature_db)\n",
    "        data_points = defaultdict(list)\n",
    "        for sample in sample_list:\n",
    "            visit_id = sample[0]\n",
    "            \n",
    "            visit_features = self.visit_features[visit_id]\n",
    "            for host, burst_pairs in visit_features['host_burst_pairs'].items():\n",
    "                for burst_pair in burst_pairs:\n",
    "                    point = [burst_pair[0], burst_pair[1], sample[0], sample[1]]\n",
    "                    data_points[self.get_second_level_domain(host)].append(point)\n",
    "        return data_points\n",
    "        \n",
    "    @classmethod\n",
    "    def write_point_files(self, data_points, out_dir):\n",
    "        #pets2014_miller_code/code/bog/extract_points.py\n",
    "        #def extract_points(feature_db, out_dir, name_file, sample_key_file, all_points = False, max_points = None):\n",
    "        #an example line from a point file:\n",
    "        #https://www.bankofamerica.com/deposits/savings/savings-accounts/ 1:198 2:3061 # 2017-11-15-12-51-12-616030.0\n",
    "        for domain, points in data_points.items():\n",
    "            with open(os.path.join(out_dir, domain), 'w') as f:\n",
    "                for (outgoing, incoming, sample_name, label) in points:\n",
    "                    f.write('%s 1:%s 2:%s # %s\\n' % (label, outgoing, incoming, sample_name))\n",
    "\n",
    "    @classmethod\n",
    "    def get_second_level_domain(self, host):\n",
    "        #pets2014_miller_code/code/feature_extractor.py\n",
    "        #def getHost(ip)\n",
    "        if not host:\n",
    "            return \"unknown\"\n",
    "        \n",
    "        tld_idx = host.rfind('.')\n",
    "        domain_idx = host.rfind('.', 0, tld_idx)\n",
    "        if domain_idx != -1:\n",
    "            host = host[(domain_idx + 1):]\n",
    "        return host\n",
    "    \n",
    "    def get_packet_size_counter_list(self, sample_list):\n",
    "        packet_size_counter_list = []\n",
    "        for sample in sample_list:\n",
    "            visit_id = sample[0]\n",
    "            \n",
    "            visit_features = self.visit_features[visit_id]\n",
    "            packet_size_counter = visit_features['packet_size_counter']\n",
    "            packet_size_counter_list.append(packet_size_counter)\n",
    "        return packet_size_counter_list\n",
    "    \n",
    "    def get_packet_size_features(self, train_list, test_list, hold_list):\n",
    "        train_counters = self.get_packet_size_counter_list(train_list)\n",
    "        test_counters = self.get_packet_size_counter_list(test_list)\n",
    "        hold_counters = self.get_packet_size_counter_list(hold_list)\n",
    "        \n",
    "        train_counters = train_counters + test_counters \n",
    "        v = DictVectorizer()\n",
    "        train_packet_size_features = v.fit_transform(train_counters)\n",
    "        test_packet_size_features = v.transform(hold_counters)\n",
    "        packet_size_features = np.row_stack([train_packet_size_features.todense(), test_packet_size_features.todense()])\n",
    "        rows, cols = packet_size_features.shape\n",
    "#         print(rows, len(train_list) + len(test_list) + len(hold_list))\n",
    "        assert rows == len(train_list) + len(test_list) + len(hold_list)\n",
    "        for i in range(cols):  # normalize\n",
    "            packet_size_features[:,i] = packet_size_features[:,i] / packet_size_features[:,i].max()\n",
    "        packet_size_features = scipy.sparse.csr_matrix(packet_size_features)\n",
    "        return packet_size_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
